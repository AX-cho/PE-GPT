{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95f04be2",
   "metadata": {},
   "source": [
    "### Initialize openai\n",
    "* 1. Provide your api_key\n",
    "* 2. Provide the api url\n",
    "* You can setup your openai keys here: https://platform.openai.com/settings/organization/billing/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5763f264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "api_key = \"Your_API_Keys_Here\"\n",
    "api_url = \"https://api.openai.com/v1\"\n",
    "openai.api_key = api_key\n",
    "api_base = api_url\n",
    "openai.base_url = api_base\n",
    "\n",
    "# # Provide the api key through the provided protal\n",
    "client = openai.OpenAI(api_key=openai.api_key, base_url=api_base)\n",
    "\n",
    "# import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3023222b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604ad9c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "668aad6e",
   "metadata": {},
   "source": [
    "### utils function\n",
    "* 1. def show_popup: show the user query and gpt's reponse in a popup window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "169e8c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tkinter_enabled = True\n",
    "\n",
    "try:\n",
    "    import tkinter as tk\n",
    "    from tkinter import Toplevel, Label, Canvas, Frame, Scrollbar\n",
    "\n",
    "    def show_popup(query, response, PE_GPT=False):\n",
    "        # Create the main application window\n",
    "        root = tk.Tk()\n",
    "        root.withdraw()  # Hide the main window as we only need the popup\n",
    "\n",
    "        # Create the popup window\n",
    "        popup = Toplevel(root)\n",
    "        popup.title(\"Scrollable Popup Window\")\n",
    "        popup.geometry(\"1000x600\")  # Set size of the popup window\n",
    "        popup.wm_attributes(\"-topmost\", True)  # Keep the popup on top\n",
    "\n",
    "        # Create a frame for the scrollable content\n",
    "        canvas = Canvas(popup)\n",
    "        scrollbar = Scrollbar(popup, orient=\"vertical\", command=canvas.yview)\n",
    "        scrollable_frame = Frame(canvas)\n",
    "\n",
    "        # Configure the canvas\n",
    "        scrollable_frame.bind(\n",
    "            \"<Configure>\",\n",
    "            lambda e: canvas.configure(scrollregion=canvas.bbox(\"all\"))\n",
    "        )\n",
    "        canvas.create_window((0, 0), window=scrollable_frame, anchor=\"nw\")\n",
    "        canvas.configure(yscrollcommand=scrollbar.set)\n",
    "\n",
    "        # Place the canvas and scrollbar\n",
    "        canvas.pack(side=\"left\", fill=\"both\", expand=True)\n",
    "        scrollbar.pack(side=\"right\", fill=\"y\")\n",
    "\n",
    "        text = f\"User: {query}\\n\\n\"\n",
    "        if PE_GPT:\n",
    "            text += f\"PE-GPT: {response}\"\n",
    "        else:\n",
    "            text += f\"GPT-4.0: {response}\"\n",
    "        label = Label(scrollable_frame, text=text, font=(\"Arial\", 18), wraplength=900, padx=20, pady=20)\n",
    "        label.pack()\n",
    "\n",
    "        # Add a button to close the popup and terminate the program\n",
    "        def close_popup():\n",
    "            popup.destroy()  # Close the popup\n",
    "            root.quit()      # Exit the main loop\n",
    "\n",
    "        # Add a button to close the popup\n",
    "        close_button = tk.Button(popup, text=\"Close\", command=close_popup, font=(\"Arial\", 16))\n",
    "        close_button.pack(pady=10)\n",
    "        # Keep the popup running\n",
    "        root.mainloop()\n",
    "        root.destroy()  # Ensure the Tkinter process is terminated after the mainloop\n",
    "except Exception as e:\n",
    "    print(f\"You got the error when importing tkinter: {e}.\")\n",
    "    print(\"You don't have to use the tkinter to demonstrate RAG and FunctionTool. It's just for better visualization.\")\n",
    "    tkinter_enabled = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c371cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2baf05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40d3324e",
   "metadata": {},
   "source": [
    "## Storyline of the Code Demo\n",
    "* RAG\n",
    "    * Stage 1. Modulation recommendation based on user request for performance metrics\n",
    "* FunctionTool\n",
    "    * Stage 2. Optimize the parameters (phase shifts) of the recommended modulation\n",
    "    * Stage 3. Validate the optimized modulation in simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48886b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4d161ec",
   "metadata": {},
   "source": [
    "### Comparison: General-Purpose GPT-4 without RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc5912c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "\n",
    "\n",
    "llm_model = \"gpt-4o-mini\" # \"gpt-4-0125-preview\" # \"gpt-3.5-turbo\" \n",
    "llm = OpenAI(model=llm_model)\n",
    "\n",
    "# Query 1: Modulation Recommendation\n",
    "query = \"Q1: I am looking for wide ZVS range with easy implementation. What modulation should I use for DAB converters?\"\n",
    "# Query 2: Information Retrival\n",
    "# query = \"Q2: For DAB converters, what are the controllable parameters in the 5-DoF strategy? Please elaborate\"\n",
    "\n",
    "\n",
    "msg = [ChatMessage(content=query, role=MessageRole.USER)]\n",
    "\n",
    "# Chat and show\n",
    "try:\n",
    "    response = llm.chat(msg)\n",
    "except Exception as e:\n",
    "    print(f\"An error occured: {e}. Check your API setup.\")\n",
    "    \n",
    "if tkinter_enabled:\n",
    "    show_popup(query, response)\n",
    "else:\n",
    "    print(\"*\"*81)\n",
    "    print(\"The user query is given below:\")\n",
    "    print(query)\n",
    "    print(\"*\"*81)\n",
    "    print(\"The response is given below:\")\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db28048d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22189386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95958e9d",
   "metadata": {},
   "source": [
    "## Stage 1: PE-GPT --- PE-Tailored GPT-4 with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1afbed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import openai\n",
    "from llama_index.core import VectorStoreIndex, ServiceContext, SimpleDirectoryReader, Document\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.node_parser import SimpleNodeParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9a6c05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_load(database_folder, llm_model, \n",
    "              temperature=None, chunk_size=None, system_prompt=None):\n",
    "    \"\"\"\n",
    "        This function is the retrieval-augmented generation (RAG) for LLM\n",
    "    \"\"\"\n",
    "    \n",
    "    if chunk_size is None: chunk_size = 1024\n",
    "    if temperature is None: temperature = 0.0\n",
    "    \n",
    "    \n",
    "    llm = OpenAI(model=llm_model, temperature=temperature, \n",
    "                 system_prompt=system_prompt)\n",
    "    \n",
    "    # Step 0. Ingest/load documents\n",
    "    docs = SimpleDirectoryReader(database_folder).load_data()\n",
    "    # Step 1. Chunking\n",
    "    node_parser = SimpleNodeParser.from_defaults(chunk_size=chunk_size)\n",
    "    # create nodes \n",
    "    nodes = node_parser.get_nodes_from_documents(docs)\n",
    "\n",
    "    # Step 2. Embedding / vectorizing, create vector database for your document \n",
    "    index = VectorStoreIndex(nodes, llm_predictor=llm)\n",
    "\n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7708be53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are PE-GPT. Please answer the user's request based on the \n",
      "documents I provided and your own knowledge. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperature, chunk_size, top_k = 0.0, 1024, 7\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are PE-GPT. Please answer the user's request based on the \n",
    "documents I provided and your own knowledge. \n",
    "\"\"\"\n",
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "090b8171",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = \"gpt-4o-mini\" # \"gpt-4-0125-preview\" # \"gpt-3.5-turbo\" \n",
    "index = rag_load(\"../core/knowledge/kb/database1\", llm_model, temperature=temperature,\n",
    "                 chunk_size=chunk_size, system_prompt=system_prompt)\n",
    "\n",
    "# create a chat_engine from the created vector database\n",
    "chat_engine = index.as_chat_engine(chat_mode=\"context\", similarity_top_k=top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "628990ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------\n",
      "System prompt is below:\n",
      "\n",
      "You are PE-GPT. Please answer the user's request based on the \n",
      "documents I provided and your own knowledge. \n",
      "\n",
      "---------------------\n",
      "Query:\n",
      "Q1: I am looking for wide ZVS range with easy implementation. What modulation should I use for DAB converters?\n"
     ]
    }
   ],
   "source": [
    "chat_prompt = f\"\"\"\n",
    "---------------------\n",
    "System prompt is below:\n",
    "{system_prompt}\n",
    "---------------------\n",
    "Query:\n",
    "\"\"\"\n",
    "# Query 1: Modulation Recommendation\n",
    "query = \"Q1: I am looking for wide ZVS range with easy implementation. What modulation should I use for DAB converters?\"\n",
    "\n",
    "# Query 2: Information Retrival\n",
    "# query = \"Q2: For DAB converters, what are the controllable parameters in the 5-DoF strategy? Please elaborate\"\n",
    "\n",
    "# Query 3: Compare the Response before and after Adding the Knowledge of PE-GPT's Initiative\n",
    "# query = \"Q3: What's the initiative of PE-GPT?\"\n",
    "\n",
    "\n",
    "chat_prompt = chat_prompt + query\n",
    "\n",
    "print(chat_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca9456e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 and Step 4: Retrieve and Query\n",
    "try:\n",
    "    response = chat_engine.chat(chat_prompt)\n",
    "except Exception as e:\n",
    "    print(f\"An error occured: {e}. Check your API setup.\")\n",
    "    \n",
    "if tkinter_enabled:\n",
    "    show_popup(query, response)\n",
    "else:\n",
    "    print(\"*\"*81)\n",
    "    print(\"The user query is given below:\")\n",
    "    print(query)\n",
    "    print(\"*\"*81)\n",
    "    print(\"The response is given below:\")\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbac5a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cffce7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f34c0fa5",
   "metadata": {},
   "source": [
    "### Stage 2 and Stage 3. FunctionTool\n",
    "* FunctionTool 1. Conduct design tasks (e.g., optimize of modulation parameters for DAB converters)\n",
    "* FunctionTool 2. Invoke plecs simulation (e.g., pass operating conditions Vin, Vout, PL and designed inner phase shift angles to simulation)\n",
    "* FunctionTool 3. Invoke multi-physics modeling with PINN (Dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45159a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 18:14:37.307 WARNING streamlit.runtime.state.session_state_proxy: Session state does not function when running a script without `streamlit run`\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.agent.openai import OpenAIAgent\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "from llama_index.llms.openai import OpenAI\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# change the working directory to the source codes of PE-GPT\n",
    "# please adjust accordingly if you want to try out\n",
    "func_folder = \"..\" # go back to the main directory of PE-GPT\n",
    "os.chdir(func_folder)\n",
    "from core.simulation.load_plecs import open_plecs, PlecsThread\n",
    "from core.optim.optimizers import optimize_mod_dab\n",
    "os.chdir(cwd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607c3000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d835220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_design_tool(Vin: float, Vout: float, PL: float) -> str:\n",
    "    \"\"\"\n",
    "        Design Task 1: Optimize modulation parameters of DAB converters under EPS modulation,\n",
    "        requires three args from user query: \n",
    "        1. an input voltage Vin (float, unit in Volt);\n",
    "        2. an output voltage Vout (float, unit in Volt);\n",
    "        3. a load power PL (float, unit in W);\n",
    "    \"\"\"\n",
    "    dab_params.Vin = Vin\n",
    "    dab_params.Vout = Vout\n",
    "    dab_params.PL = PL\n",
    "    \n",
    "    ipp, ZVS, ZCS, PL, pos, plot, modulation = optimize_mod_dab(Vin, Vout, PL, \"EPS\")\n",
    "    if modulation == \"EPS1\":\n",
    "        dab_params.D1, dab_params.D2 = float(pos[1]), 1.\n",
    "    elif modulation == \"EPS2\":\n",
    "        dab_params.D2, dab_params.D1 = float(pos[1]), 1.\n",
    "    \n",
    "    response = f\"\"\"The optimized modulation parameters for the EPS modulation under PL = {PL:.1f} W, \n",
    "    Vin = {Vin:.1f} V, Vout = {Vout:.1f} V are: {[item.round(3) for item in pos]}. The evaluated \n",
    "    peak-to-peak current is {ipp:.2f} A, number of ZVS switches is: {ZVS}, number of ZCS switches is {ZCS}.\"\"\"\n",
    "    return response, plot\n",
    "\n",
    "\n",
    "def circuit_simulation_tool():    # chat_engine, prompt, messages_history\n",
    "    \"\"\"\n",
    "        Design Task 2: Invoke/call/interface circuit simulation for DAB converters\n",
    "    \"\"\"\n",
    "    print(\"Executing Task circuit_simulation_tool.\")\n",
    "    \n",
    "    model_name = \"DAB\"\n",
    "    file_path = os.path.abspath(f\"../core/simulation/{model_name}.plecs\")\n",
    "    open_plecs(file_path)\n",
    "    \n",
    "    kwargs = {\"Vin\": dab_params.Vin, \"Vref\": dab_params.Vout, \"P\": dab_params.PL, \n",
    "              \"D1\": dab_params.D1, \"D2\": dab_params.D2, \n",
    "              \"Ro\": dab_params.Vout**2/dab_params.PL, \"phi1\": 0., \"phi2\": 0.,}\n",
    "    print(kwargs)\n",
    "    # conduct the plecs simulation\n",
    "    thread = PlecsThread(model_name, file_path, **kwargs)\n",
    "    thread.start()\n",
    "    \n",
    "    response = \"Plecs simulation for designed DAB converters is conducting. Please wait a while...\"\n",
    "    return response\n",
    "    \n",
    "    \n",
    "def multi_physics_tool():\n",
    "    \"\"\"\n",
    "        Design Task 3: Invoke/call/interface multi-physics (electrical, magnetic, \n",
    "        thermal, mechanical) simulation of designed components.\n",
    "    \"\"\"\n",
    "    print(\"Executing Task multi_physics_tool.\")\n",
    "\n",
    "\n",
    "conduct_design_tool = FunctionTool.from_defaults(fn=conduct_design_tool)\n",
    "circuit_simulation_tool = FunctionTool.from_defaults(fn=circuit_simulation_tool)\n",
    "multi_physics_tool = FunctionTool.from_defaults(fn=multi_physics_tool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b99b9654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAB_Params(Vin=0.0, Vout=0.0, PL=0.0, phi1=0.0, phi2=0.0, D1=0.0, D2=0.0)\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "# define a class to store some key hyperparameters of DAB converters \n",
    "# Pass the parameters to simulation \n",
    "@dataclass\n",
    "class DAB_Params:\n",
    "    Vin: float = 0.\n",
    "    Vout: float = 0.\n",
    "    PL: float = 0.\n",
    "    phi1: float = 0.\n",
    "    phi2: float = 0.\n",
    "    D1: float = 0.\n",
    "    D2: float = 0.\n",
    "\n",
    "dab_params = DAB_Params()\n",
    "print(dab_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "378b3342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: \n",
      "Please invoke corresponding function based on user query.\n",
      "Note that only one function will be called.\n",
      "---------------------------------\n",
      "User Query: Please help me design the EPS modulation for DAB converters.\n",
      "    My Vi is 0.3 kv, output voltage is 360 V, load power is 200 W.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 18:14:41,753 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: conduct_design_tool with args: {\"Vin\":300,\"Vout\":360,\"PL\":200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 18:14:42.510 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-03-17 18:14:42,560 - pyswarms.single.global_best - INFO - Optimize for 50 iters with {'c1': 2.05, 'c2': 2.05, 'w': 0.9}\n",
      "pyswarms.single.global_best: 100%|██████████████████████████████████████████████████████████████|50/50, best_cost=-6.66\n",
      "2025-03-17 18:15:28,071 - pyswarms.single.global_best - INFO - Optimization finished | best cost: -6.663459777832031, best pos: [-0.15702949  0.66200147]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got output: ('The optimized modulation parameters for the EPS modulation under PL = 200.0 W, \\n    Vin = 300.0 V, Vout = 360.0 V are: [-0.157, 0.662]. The evaluated \\n    peak-to-peak current is 10.20 A, number of ZVS switches is: 8.0, number of ZCS switches is 0.0.', <_io.BytesIO object at 0x000001E60C164F90>)\n",
      "========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 18:15:31,364 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAB_Params(Vin=300, Vout=360, PL=200, phi1=0.0, phi2=0.0, D1=1.0, D2=0.662)\n"
     ]
    }
   ],
   "source": [
    "# Create a simplified PE-GPT agent with FunctionTool routing capability\n",
    "# define task_agent which routes the user query to different tools\n",
    "def task_agent(llm_model: str, tools: List[FunctionTool]):\n",
    "    \"\"\"\n",
    "        Define an LLM agent to judge and keep track of the design stage/task/workflow\n",
    "    \"\"\"\n",
    "\n",
    "    llm = OpenAI(model=llm_model)\n",
    "    agent = OpenAIAgent.from_tools(\n",
    "        tools, llm=llm, verbose=True)\n",
    "    \n",
    "    return agent\n",
    "\n",
    "\n",
    "llm_model = \"gpt-4o-mini\" # \"gpt-4-0125-preview\" # \"gpt-3.5-turbo\" \n",
    "tools = [conduct_design_tool,\n",
    "         circuit_simulation_tool,\n",
    "         multi_physics_tool]\n",
    "llm = task_agent(llm_model, tools)\n",
    "\n",
    "# Query for conducting modulation optimization\n",
    "query = \"\"\"Please help me design the EPS modulation for DAB converters.\n",
    "    My Vi is 0.3 kv, output voltage is 360 V, load power is 200 W.\"\"\"\n",
    "\n",
    "\n",
    "response = llm.chat(f\"\"\"\n",
    "Please invoke corresponding function based on user query.\n",
    "Note that only one function will be called.\n",
    "---------------------------------\n",
    "User Query: {query}\"\"\")\n",
    "\n",
    "print(dab_params)\n",
    "if tkinter_enabled:\n",
    "    show_popup(query, response)\n",
    "else:\n",
    "    print(\"*\"*81)\n",
    "    print(\"The user query is given below:\")\n",
    "    print(query)\n",
    "    print(\"*\"*81)\n",
    "    print(\"The response is given below:\")\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c7e43b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# Assuming you already have a BytesIO object named `buffer` with a plot saved in it\n",
    "# buffer.seek(0) ensures you're at the start of the BytesIO stream\n",
    "# buffer.seek(0)\n",
    "\n",
    "# Open the image using PIL\n",
    "image = Image.open(response.sources[0].raw_output[1])\n",
    "\n",
    "# Display the image\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09d877f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: \n",
      "Please invoke corresponding function based on user query.\n",
      "Note that only one function will be called.\n",
      "---------------------------------\n",
      "User Query: Can you simulate the designed modulation for DAB converter?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 18:15:44,218 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: circuit_simulation_tool with args: {}\n",
      "Executing Task circuit_simulation_tool.\n",
      "{'Vin': 300, 'Vref': 360, 'P': 200, 'D1': 1.0, 'D2': 0.662, 'Ro': 648.0, 'phi1': 0.0, 'phi2': 0.0}\n",
      "Got output: Plecs simulation for designed DAB converters is conducting. Please wait a while...\n",
      "========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 18:15:45,458 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Query for verifying the designed modulation in simulation\n",
    "query = \"Can you simulate the designed modulation for DAB converter?\"\n",
    "# query = \"Can you simulate the temperature distribution field?\"\n",
    "\n",
    "response = llm.chat(f\"\"\"\n",
    "Please invoke corresponding function based on user query.\n",
    "Note that only one function will be called.\n",
    "---------------------------------\n",
    "User Query: {query}\"\"\")\n",
    "\n",
    "if tkinter_enabled:\n",
    "    show_popup(query, response)\n",
    "else:\n",
    "    print(\"*\"*81)\n",
    "    print(\"The user query is given below:\")\n",
    "    print(query)\n",
    "    print(\"*\"*81)\n",
    "    print(\"The response is given below:\")\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b8ff70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a1c772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8f552e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f03701",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
